{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (using entropy and confidence level of 0%):   100.0\n",
      "   Number correctly classified:    2031\n",
      "   Number incorrectly classified:  0\n",
      "Accuracy (using entropy and confidence level of 99%):  100.0\n",
      "   Number correctly classified:    2031\n",
      "   Number incorrectly classified:  0\n",
      "Accuracy (using entropy and confidence level of 95%):  100.0\n",
      "   Number correctly classified:    2031\n",
      "   Number incorrectly classified:  0\n",
      "Accuracy (using entropy and confidence level of 50%):   100.0\n",
      "   Number correctly classified:    2031\n",
      "   Number incorrectly classified:  0\n",
      "\n",
      "Accuracy (using misclassification and confidence level of 0%):   100.0\n",
      "   Number correctly classified:    2031\n",
      "   Number incorrectly classified:  0\n",
      "Accuracy (using misclassification and confidence level of 99%):  100.0\n",
      "   Number correctly classified:    2031\n",
      "   Number incorrectly classified:  0\n",
      "Accuracy (using misclassification and confidence level of 95%):  100.0\n",
      "   Number correctly classified:    2031\n",
      "   Number incorrectly classified:  0\n",
      "Accuracy (using misclassification and confidence level of 50%):   100.0\n",
      "   Number correctly classified:    2031\n",
      "   Number incorrectly classified:  0\n",
      "\n",
      "best tree:  id3_entropy_confidence_level_0\n",
      "\n",
      "classification predictions on validation data written to: validation-best-accuracy.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using pandas DataFrame to hold training, testing, and validation data\n",
    "import pandas as pd\n",
    "from pandas import set_option\n",
    "set_option(\"display.max_rows\", 20)\n",
    "\n",
    "LARGE_FIGSIZE = (12, 8)\n",
    "import math\n",
    "\n",
    "names_of_attributes = names=[\"label\",\"cap-shape\",\"cap-surface\",\"cap-color\",\"bruises\",\"oder\",\n",
    "                                \"gill-attachment\",\"gill-spacing\",\"gill-size\",\"gill-color\",\"stalk-shape\",\n",
    "                                \"stalk-root\",\"stalk-surface-above-ring\",\"stalk-surface-below-ring\",\n",
    "                                \"stalk-color-above-ring\",\"stalk-color-below-ring\",\"veil-type\",\n",
    "                                \"veil-color\",\"ring-number\",\"ring-type\",\"spore-print-color\",\n",
    "                                \"population\",\"habitat\"]\n",
    "\n",
    "#read training data into pandas DataFrame\n",
    "training_data = pd.read_table(\"data/training.txt\", sep=\",\", names=names_of_attributes)\n",
    "\n",
    "#read testing data into pandas DataFrame\n",
    "testing_data = pd.read_table(\"data/testing.txt\", sep=\",\", names=names_of_attributes)\n",
    "\n",
    "\n",
    "#represents a decision node in the tree\n",
    "class Node:\n",
    "    #identifies which attribute this node tests on\n",
    "    decision_attribute = \"\"\n",
    "    \n",
    "    #initialize node\n",
    "    def __init__(self):\n",
    "        self.decision_attribute = \"\"\n",
    "        self.branches = {}\n",
    "        \n",
    "    #adds a child Node or Leaf \n",
    "    def add_branch(self,attribute_value,node):\n",
    "        self.branches[attribute_value] = node\n",
    "    \n",
    "    #returns all branches\n",
    "    def get_branches(self):\n",
    "        return self.branches\n",
    "\n",
    "#represents a leaf node in the tree\n",
    "class Leaf:\n",
    "    #label representing the classification value (e or p)\n",
    "    label = \"\"\n",
    "\n",
    "#global variables for the positive and negative label values (e and p)    \n",
    "positive_value = \"e\"\n",
    "negative_value = \"p\"\n",
    "\n",
    "#global variable for setting the evaluation criteria (entropy or misclassification)\n",
    "#can be set just before calling the id3 algorithm below\n",
    "evaluation_criteria = \"entropy\"\n",
    "\n",
    "#look-up table for the chi-square values\n",
    "#key = dof, values[0] = alpha 0.5, values[1] =  alpha 0.05, values[2] = alpha 0.01\n",
    "chi_square_look_up = {\n",
    "                    1: [0.4549,3.841,6.635],\n",
    "                    2: [1.3863,5.991,9.21],\n",
    "                    3: [2.366,7.815,11.345],\n",
    "                    4: [3.3567,9.488,13.277],\n",
    "                    5: [4.3515,11.07,15.086],\n",
    "                    6: [5.3481,12.592,16.812],\n",
    "                    7: [6.3458,14.067,18.475],\n",
    "                    8: [7.3441,15.507,20.09],\n",
    "                    9: [8.3428,16.919,21.666],\n",
    "                    10: [9.3418,18.307,23.209],\n",
    "                    11: [10.341,19.675,24.725],\n",
    "                    12: [11.3403,21.026,26.217],\n",
    "                    13: [12.3398,22.362,27.688],\n",
    "                    14: [13.3393,23.685,29.141],\n",
    "                    15: [14.3389,24.996,30.578],\n",
    "                    16: [15.3385,26.296,32],\n",
    "                    17: [16.3382,27.587,33.409],\n",
    "                    18: [17.3379,28.869,34.805],\n",
    "                    19: [18.3377,30.144,36.191],\n",
    "                    20: [19.3374,31.41,37.566]\n",
    "                    }\n",
    "\n",
    "#look-up table for the valid values of each attribute\n",
    "valid_values = {\"label\": [\"p\",\"e\"],\n",
    "                \"cap-shape\": [\"b\",\"c\",\"x\",\"f\",\"k\",\"s\"],\n",
    "                \"cap-surface\": [\"f\",\"g\",\"y\",\"s\"],\n",
    "                \"cap-color\": [\"n\",\"b\",\"c\",\"g\",\"r\",\"p\",\"u\",\"e\",\"w\",\"y\"],\n",
    "                \"bruises\": [\"t\",\"f\"],\n",
    "                \"oder\": [\"a\",\"l\",\"c\",\"y\",\"f\",\"m\",\"n\",\"p\",\"s\"],\n",
    "                \"gill-attachment\": [\"a\",\"d\",\"f\",\"n\"],\n",
    "                \"gill-spacing\": [\"c\",\"w\",\"d\"],\n",
    "                \"gill-size\": [\"b\",\"n\"],\n",
    "                \"gill-color\": [\"k\",\"n\",\"b\",\"h\",\"g\",\"r\",\"o\",\"p\",\"u\",\"e\",\"w\",\"y\"],\n",
    "                \"stalk-shape\": [\"e\",\"t\"],\n",
    "                \"stalk-root\": [\"b\",\"c\",\"u\",\"e\",\"z\",\"r\",\"?\"],\n",
    "                \"stalk-surface-above-ring\": [\"f\",\"y\",\"k\",\"s\"],\n",
    "                \"stalk-surface-below-ring\": [\"f\",\"y\",\"k\",\"s\"],\n",
    "                \"stalk-color-above-ring\": [\"n\",\"b\",\"c\",\"g\",\"o\",\"p\",\"e\",\"w\",\"y\"],\n",
    "                \"stalk-color-below-ring\": [\"n\",\"b\",\"c\",\"g\",\"o\",\"p\",\"e\",\"w\",\"y\"],\n",
    "                \"veil-type\": [\"p\",\"u\"],\n",
    "                \"veil-color\": [\"n\",\"o\",\"w\",\"y\"],\n",
    "                \"ring-number\": [\"n\",\"o\",\"t\"],\n",
    "                \"ring-type\": [\"c\",\"e\",\"f\",\"l\",\"n\",\"p\",\"s\",\"z\"],\n",
    "                \"spore-print-color\": [\"k\",\"n\",\"b\",\"h\",\"r\",\"o\",\"u\",\"w\",\"y\"],\n",
    "                \"population\": [\"a\",\"c\",\"n\",\"s\",\"v\",\"y\"],\n",
    "                \"habitat\": [\"g\",\"l\",\"m\",\"p\",\"u\",\"w\",\"d\"]\n",
    "                }\n",
    "\n",
    "#calculates the degrees of freedom for an attribute\n",
    "def degress_of_freedom(attribute):\n",
    "    return len(valid_values[attribute]) - 1\n",
    "\n",
    "#returns the chi-square value given an attribute and alpha value\n",
    "def chi_square_value_for_attribute(attribute,alpha):\n",
    "    values = chi_square_look_up[degress_of_freedom(attribute)]\n",
    "    if alpha == '0.5':\n",
    "        return values[0]\n",
    "    elif alpha == '0.05':\n",
    "        return values[1]\n",
    "    elif alpha == '0.01':\n",
    "        return values[2]\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "#returns the size (number of rows) of a DataFrame\n",
    "def size(data):\n",
    "    return len(data.index)\n",
    "\n",
    "#returns a subset of a DataFrame, filtering on the value of an attribute\n",
    "def filter_data(examples,attribute, value):\n",
    "    return examples[examples[attribute] == value]\n",
    "    \n",
    "#tests if all the elements in the DataFrame (examples) have same value for an attribute    \n",
    "def all_one_value(examples, attribute, attribute_value):\n",
    "    filtered_examples = filter_data(examples, attribute, attribute_value)\n",
    "    return size(filtered_examples) == size(examples)\n",
    "\n",
    "#returns all possible values for an attribute\n",
    "def possible_range_of_values(attribute):\n",
    "    return valid_values[attribute]\n",
    "\n",
    "#returns the most common value present the the examples for a given attribute\n",
    "def extract_most_common_value(examples, attribute):\n",
    "    values = possible_range_of_values(attribute)\n",
    "    most_common = \"\"\n",
    "    highest_count = 0\n",
    "    for value in values:\n",
    "        filtered = filter_data(examples, attribute, value)\n",
    "        size_filtered = size(filtered)\n",
    "        if size_filtered > highest_count:\n",
    "            highest_count = size_filtered\n",
    "            most_common = value  \n",
    "    return most_common\n",
    "\n",
    "#sub-calculation for chi-square, calculates the chi-square for an attribute value\n",
    "def chi_square_calculation_attribute_v(examples_v, target_attribute,attribute_v):\n",
    "    total_count = size(examples_v)\n",
    "    #if the sizie of examples_v is zero then there is nothing to calculate and return 0\n",
    "    if total_count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    positive_filtered_examples = filter_data(examples_v, target_attribute, positive_value)\n",
    "    negative_filtered_examples = filter_data(examples_v, target_attribute, negative_value)\n",
    "    \n",
    "    observed_positive = size(positive_filtered_examples)\n",
    "    expected_positive = (total_count/2.0)\n",
    "                \n",
    "    observed_negative = size(negative_filtered_examples)\n",
    "    expected_negative = (total_count/2.0)\n",
    "    \n",
    "    pos = ((observed_positive - expected_positive)**2)/expected_positive\n",
    "    neg = ((observed_negative - expected_negative)**2)/expected_negative\n",
    "    return pos + neg\n",
    "          \n",
    "#returns the chi-square value for an attribute    \n",
    "def chi_square_calculation(examples, target_attribute,attribute):\n",
    "    chi_square_sum = 0.0\n",
    "    values = possible_range_of_values(attribute)\n",
    "    for value in values:\n",
    "        examples_v = filter_data(examples, attribute,value)\n",
    "        chi_square_v = chi_square_calculation_attribute_v(examples_v,target_attribute,value)\n",
    "        chi_square_sum = chi_square_sum + chi_square_v\n",
    "    return chi_square_sum\n",
    "    \n",
    "#calculates the entropy on a subset (examples_v) of data\n",
    "def calculate_entropy(examples_v,target_attribute):\n",
    "    entropy = 0.0\n",
    "    total_size = size(examples_v)\n",
    "    #if there is no data just return a zero value (does not contribute to calculation)\n",
    "    if total_size == 0:\n",
    "        return 0.0\n",
    "    positive_filtered_examples_v = filter_data(examples_v, target_attribute, positive_value)\n",
    "    negative_filtered_examples_v = filter_data(examples_v, target_attribute, negative_value)\n",
    "    \n",
    "    num_positive = size(positive_filtered_examples_v)\n",
    "    calc_positive = 0.0\n",
    "    if num_positive != 0:\n",
    "        calc_positive = -(num_positive/total_size)*math.log((num_positive/total_size),2)\n",
    "            \n",
    "        \n",
    "    num_negative = size(negative_filtered_examples_v)\n",
    "    calc_negative = 0.0\n",
    "    if num_negative != 0:\n",
    "        calc_negative = - (num_negative/total_size)*math.log((num_negative/total_size),2)\n",
    "        \n",
    "    #returns -p(+)log p(+) - p(-)log p(-)    \n",
    "    return calc_positive + calc_negative\n",
    "   \n",
    "#calculates the misclassification error on a subset (examples_v) of data\n",
    "def calculate_misclassification_error(examples_v, target_attribute):\n",
    "    error = 0.0\n",
    "    total_size = size(examples_v)\n",
    "    #if there is no data just return a zero value (does not contribute to calculation)\n",
    "    if total_size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    positive_filtered_examples_v = filter_data(examples_v,target_attribute,positive_value)\n",
    "    negative_filtered_examples_v = filter_data(examples_v,target_attribute,negative_value)\n",
    "    \n",
    "    num_positive = size(positive_filtered_examples_v)\n",
    "    p_positive = (num_positive/total_size)\n",
    "                \n",
    "    num_negative = size(negative_filtered_examples_v)\n",
    "    p_negative = (num_negative/total_size)\n",
    "    \n",
    "    error = 1.0 - max([p_positive,p_negative])\n",
    "    #returns 1 - max(probability of positive,probability of negative)\n",
    "    return error\n",
    "    \n",
    "#returns the attribute with the best informatin gain using entropy as the impurity measure\n",
    "def determine_best_attribute_entropy(examples, target_attribute, attributes):\n",
    "    best_attribute = \"\"\n",
    "    max_information_gain = 0.0\n",
    "   \n",
    "    #find the entropy of S (examples)\n",
    "    entropy_s = calculate_entropy(examples,target_attribute)\n",
    "    size_of_s = size(examples)\n",
    "    \n",
    "    \n",
    "    #loop through attributes and calculate the gain\n",
    "    for attribute in attributes:\n",
    "        values = possible_range_of_values(attribute)\n",
    "        information_gain = entropy_s\n",
    "        for value in values:\n",
    "            examples_v = filter_data(examples,attribute,value)\n",
    "            entropy_v = calculate_entropy(examples_v,target_attribute)\n",
    "            size_of_examples_v = size(examples_v)\n",
    "            weight = (size_of_examples_v/size_of_s)\n",
    "            information_gain = information_gain - weight*entropy_v\n",
    "        #if we have a new candidate for the best attribute store the gain and attribute name\n",
    "        if information_gain > max_information_gain:\n",
    "            max_information_gain = information_gain\n",
    "            best_attribute = attribute\n",
    "             \n",
    "    return best_attribute\n",
    "\n",
    "#returns the attribute with the best information gain using misclassification error as the impurity measure\n",
    "def determine_best_attribute_misclassification_error(examples, target_attribute, attributes):\n",
    "    best_attribute = \"\"\n",
    "    max_information_gain = 0.0\n",
    "    #find the misclassification error of S (examples)\n",
    "    error_s = calculate_misclassification_error(examples,target_attribute)\n",
    "    size_of_s = size(examples)\n",
    "    \n",
    "    #loop through attributes and calculate the gain\n",
    "    for attribute in attributes:\n",
    "        values = possible_range_of_values(attribute)\n",
    "        information_gain = error_s\n",
    "        for value in values:\n",
    "            examples_v = filter_data(examples,attribute,value)\n",
    "            error_v = calculate_misclassification_error(examples_v,target_attribute)\n",
    "            size_of_examples_v = size(examples_v)\n",
    "            weight = (size_of_examples_v/size_of_s)\n",
    "            information_gain = information_gain - weight*error_v\n",
    "        #if we have a new candidate for the best attribute store the gain and attribute name\n",
    "        if information_gain > max_information_gain:\n",
    "            max_information_gain = information_gain\n",
    "            best_attribute = attribute\n",
    "       \n",
    "    return best_attribute\n",
    "\n",
    "#performs the id3 decision tree algorithm (recursive), returns the root node of the tree\n",
    "#  examples - set (pandas DataFrame) of training examples provided for current iteration of the algorithm\n",
    "#  target_attribute - the name of the column that provides the classification label\n",
    "#  attribute_list - set of the names of the attributes for the training examples (excludes target_attribute)\n",
    "#  alpha - the alpha value [\"0.01\",\"0.05\",\"0.5\",\"0.0\"] used for chi-square pruning \n",
    "def id3(examples, target_attribute, attribute_list, alpha):\n",
    "    root = Node()\n",
    "    if(all_one_value(examples, target_attribute, positive_value)):\n",
    "        leaf = Leaf()\n",
    "        leaf.label = positive_value\n",
    "        return leaf\n",
    "    if(all_one_value(examples, target_attribute, negative_value)):\n",
    "        leaf = Leaf()\n",
    "        leaf.label = negative_value\n",
    "        return leaf\n",
    "    if(len(attribute_list) == 0):\n",
    "        leaf = Leaf()\n",
    "        leaf.label = extract_most_common_value(examples,target_attribute)\n",
    "        return leaf\n",
    "    #evaluate node selection using entropy or misclassification error\n",
    "    if evaluation_criteria == 'entropy':\n",
    "        a = determine_best_attribute_entropy(examples, target_attribute, attribute_list)\n",
    "    else:\n",
    "        a = determine_best_attribute_misclassification_error(examples, target_attribute, attribute_list)\n",
    "    \n",
    "    #for pruning get the calculated and threshold chi-square value\n",
    "    chi_square_calculated = chi_square_calculation(examples,target_attribute,a)\n",
    "    chi_square_lookup = chi_square_value_for_attribute(a, alpha)\n",
    "    \n",
    "    #if the calculated value is less than the threshold value then prune (decision node -> leaf node)\n",
    "    if chi_square_calculated < chi_square_lookup:\n",
    "        leaf = Leaf()\n",
    "        leaf.label = extract_most_common_value(examples,target_attribute)\n",
    "        return leaf\n",
    "    root.decision_attribute = a\n",
    "   \n",
    "    values = possible_range_of_values(a)\n",
    "    #iterate over all the possible values of an attribute and create branches for each value\n",
    "    for value in values:\n",
    "        root.branches[value] = \"\"\n",
    "        examples_v = filter_data(examples,a,value)\n",
    "        size_of_examples_v = size(examples_v)\n",
    "        #if there are no more examples to train on create a leaf node for this branch\n",
    "        if size_of_examples_v == 0:\n",
    "            leaf = Leaf()\n",
    "            leaf.label = extract_most_common_value(examples,target_attribute)\n",
    "            root.add_branch(value,leaf)\n",
    "        else:\n",
    "            #remove attribute a so we don't reprocess it\n",
    "            if a in attribute_list:\n",
    "                attribute_list.remove(a)\n",
    "            #add the next best decision node or leaf\n",
    "            root.add_branch(value, id3(examples_v,target_attribute,attribute_list, alpha))\n",
    "    return root\n",
    "\n",
    "\n",
    "\n",
    "#traverses the decision tree one level and returns the node or leaf at the end of the selected branch\n",
    "#  example_attribute_values - the example's attribute values <v1,v2,...vn>\n",
    "#  node - decision node\n",
    "def split_child(example_attribute_values, node):\n",
    "    #from the example get the value of the attribute associated with is node example\n",
    "    example_decision_attribute_value = example_attribute_values[node.decision_attribute]\n",
    "    #return the node or leaf of the branch associated with the example's value of the decision attribute \n",
    "    return node.get_branches()[example_decision_attribute_value]\n",
    "\n",
    "#classify an example, return the value of the first leaf node discovered\n",
    "#  example_attribute_values - the example's attribute values <v1,v2,...vn>\n",
    "#  node - decision node\n",
    "def classify(example_attribute_values, node):\n",
    "    if type(node) is Leaf:\n",
    "        return node.label\n",
    "    else:\n",
    "        return classify(example_attribute_values, split_child(example_attribute_values,node))\n",
    "    \n",
    "#returns the accuracy of classifing on a testing data set\n",
    "#  testing_data - the data set to classify (pandas DataFrame)\n",
    "#  id3_root - the root of the id3 decision tree\n",
    "#  print_statement - the string to print\n",
    "def calculate_accuracy(testing_data, id3_root, print_statement):\n",
    "    count_correct = 0\n",
    "    count_false = 0\n",
    "    accuracy = 0.0\n",
    "    for i,r in testing_data.iterrows():\n",
    "        #if the classifing label in the data set matches the calculated one we classified correctly\n",
    "        if r[\"label\"] == classify(r,id3_root):\n",
    "            count_correct = count_correct + 1\n",
    "        else:\n",
    "            count_false = count_false + 1\n",
    "    if count_correct > 0:\n",
    "        accuracy = (count_correct)/(count_correct + count_false)  * 100\n",
    "    \n",
    "    print(print_statement, accuracy)\n",
    "    print(\"  \", \"Number correctly classified:   \", count_correct) \n",
    "    print(\"  \", \"Number incorrectly classified: \", count_false) \n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "#helper function for getting the list of attributes for the data sets\n",
    "def get_attributes():\n",
    "    \n",
    "    #list of attributes to be used in the id3 algorithm. \n",
    "    #After an attribute is selected for a node of the tree it is removed from the list\n",
    "    attributes = [\"cap-shape\",\"cap-surface\",\"cap-color\",\"bruises\",\"oder\",\n",
    "                \"gill-attachment\",\"gill-spacing\",\"gill-size\",\"gill-color\",\"stalk-shape\",\n",
    "                \"stalk-root\",\"stalk-surface-above-ring\",\"stalk-surface-below-ring\",\n",
    "                \"stalk-color-above-ring\",\"stalk-color-below-ring\",\"veil-type\",\n",
    "                \"veil-color\",\"ring-number\",\"ring-type\",\"spore-print-color\",\n",
    "                \"population\",\"habitat\"]\n",
    "    return attributes\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_tree = Node()\n",
    "best_tree_name = ''\n",
    "\n",
    "#the following group of statements run the id3 algorithm for different values of evaluation criteria and alpha (for pruning)\n",
    "evaluation_criteria = \"entropy\"\n",
    "alpha = '1.0'\n",
    "print_statement = \"Accuracy (using entropy and confidence level of 0%):  \"\n",
    "id3_entropy = id3(training_data,\"label\",get_attributes(),alpha)\n",
    "accuracy = id3_entropy_accuracy = calculate_accuracy(testing_data,id3_entropy, print_statement)\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_tree = id3_entropy\n",
    "    best_tree_name = 'id3_entropy_confidence_level_0'\n",
    "\n",
    "evaluation_criteria = \"entropy\"\n",
    "alpha = '0.01'\n",
    "print_statement = \"Accuracy (using entropy and confidence level of 99%): \"\n",
    "id3_entropy_alpha_01 = id3(training_data,\"label\",get_attributes(),alpha)\n",
    "accuracy = id3_entropy_alpha_01_accuracy= calculate_accuracy(testing_data,id3_entropy_alpha_01, print_statement)\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_tree = id3_entropy_alpha_01\n",
    "    best_tree_name = 'id3_entropy_confidence_level_99'\n",
    "\n",
    "evaluation_criteria = \"entropy\"\n",
    "alpha = '0.05'\n",
    "print_statement = \"Accuracy (using entropy and confidence level of 95%): \"\n",
    "id3_entropy_alpha_05 = id3(training_data,\"label\",get_attributes(),alpha)\n",
    "accuracy = id3_entropy_alpha_05_accuracy = calculate_accuracy(testing_data,id3_entropy_alpha_05, print_statement)\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_tree = id3_entropy_alpha_05\n",
    "    best_tree_name = 'id3_entropy_confidence_level_95'\n",
    "\n",
    "evaluation_criteria = \"entropy\"\n",
    "alpha = '0.5'\n",
    "print_statement = \"Accuracy (using entropy and confidence level of 50%):  \"\n",
    "id3_entropy_alpha_5 = id3(training_data,\"label\",get_attributes(),alpha)\n",
    "accuracy = id3_entropy_alpha_5_accuracy = calculate_accuracy(testing_data,id3_entropy_alpha_5, print_statement)\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_tree = id3_entropy_alpha_5\n",
    "    best_tree_name = 'id3_entropy_confidence_level_50'\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "evaluation_criteria = \"misclassification\"\n",
    "alpha = '1.0'\n",
    "print_statement = \"Accuracy (using misclassification and confidence level of 0%):  \"\n",
    "id3_misclassification_error = id3(training_data,\"label\",get_attributes(),alpha)\n",
    "accuracy = calculate_accuracy(testing_data,id3_misclassification_error, print_statement)\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    print(accuracy, best_accuracy)\n",
    "    best_tree = id3_misclassification_error\n",
    "    best_tree_name = 'id3_misclassification_error_confidence_level_0'\n",
    "\n",
    "evaluation_criteria = \"misclassification\"\n",
    "alpha = '0.01'\n",
    "print_statement = \"Accuracy (using misclassification and confidence level of 99%): \"\n",
    "id3_misclassification_error_alpha01 = id3(training_data,\"label\",get_attributes(),alpha)\n",
    "accuracy = calculate_accuracy(testing_data,id3_misclassification_error_alpha01, print_statement)\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_tree = id3_misclassification_error_alpha01\n",
    "    best_tree_name = 'id3_misclassification_error_confidence_level_99'\n",
    "\n",
    "evaluation_criteria = \"misclassification\"\n",
    "alpha = '0.05'\n",
    "print_statement = \"Accuracy (using misclassification and confidence level of 95%): \"\n",
    "id3_misclassification_error_alpha05 = id3(training_data,\"label\",get_attributes(),alpha)\n",
    "accuracy = calculate_accuracy(testing_data,id3_misclassification_error_alpha05, print_statement)\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_tree = id3_misclassification_error_alpha05\n",
    "    best_tree_name = 'id3_misclassification_error_confidence_level_95'\n",
    "\n",
    "evaluation_criteria = \"misclassification\"\n",
    "alpha = '0.5'\n",
    "print_statement = \"Accuracy (using misclassification and confidence level of 50%):  \"\n",
    "id3_misclassification_error_alpha5 = id3(training_data,\"label\",get_attributes(),alpha)\n",
    "accuracy = calculate_accuracy(testing_data,id3_misclassification_error_alpha5, print_statement)\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_tree = id3_misclassification_error_alpha5\n",
    "    best_tree_name = 'id3_misclassification_error_confidence_level_50'\n",
    "    \n",
    "\n",
    "print(\"\")    \n",
    "print(\"best tree: \", best_tree_name)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "#read validation data into pandas DataFrame    \n",
    "validation_data = pd.read_table(\"data/validation.txt\", sep=\",\", names=names_of_attributes)\n",
    "\n",
    "\n",
    "file = open(\"validation-best-accuracy.txt\", \"w\")\n",
    "for i,r in validation_data.iterrows():\n",
    "    file.write(classify(r,best_tree) + \"\\n\")\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(\"classification predictions on validation data written to: validation-best-accuracy.txt\")\n",
    "\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
